#!/usr/bin/env bash
set -euo pipefail

# Configurable endpoints (override via env)
# When running on ddev network, use 'web' service name instead of 'zms.ddev.site'
# When running on podman, use 'zms-web' service name
# BASE_URI should include the full path prefix and be HTTP to avoid TLS trust issues inside container
BASE_URI="${BASE_URI:-}"
# CITIZEN_API_BASE_URI should include full path prefix; both APIs are served by the same site
CITIZEN_API_BASE_URI="${CITIZEN_API_BASE_URI:-}"

# Maven image and caches
MAVEN_IMAGE="maven:3.9-eclipse-temurin-17"
M2_CACHE_VOLUME="zmsautomation-cache"
TARGET_VOLUME="zmsautomation-target"
WORKSPACE_ROOT="$(cd "$(dirname "$0")"/.. && pwd)"
PROJECT_ROOT="$WORKSPACE_ROOT"

# Temporary backup file inside host
BACKUP_FILE="/tmp/zms_backup_$(date +%s).sql.gz"
BASE_SQL_FILE="$PROJECT_ROOT/.resources/zms.sql"

require_cmd() {
  command -v "$1" >/dev/null 2>&1 || { echo "ERROR: '$1' is required but not installed." >&2; exit 1; }
}

info() { echo "[INFO] $*"; }
warn() { echo "[WARN] $*"; }

# Detect if we're using Podman or DDEV
USE_PODMAN=false
info "[DEBUG] Starting Podman detection..."
info "[DEBUG] PATH: ${PATH}"
info "[DEBUG] Checking for podman command..."
PODMAN_CMD=$(command -v podman 2>&1 || echo "NOT_FOUND")
info "[DEBUG] command -v podman result: '${PODMAN_CMD}'"

# Try direct podman call as fallback
if [[ "$PODMAN_CMD" != "NOT_FOUND" ]] || podman --version >/dev/null 2>&1; then
  info "[DEBUG] Podman command found, checking for containers..."
  # Use the full path to podman if available
  PODMAN_EXEC="${PODMAN_CMD:-podman}"
  info "[DEBUG] Using podman at: ${PODMAN_EXEC}"
  
  # Check for both required containers, handling potential whitespace issues
  # Try multiple approaches to get container names (format flag might not work in all contexts)
  CONTAINER_NAMES=""
  
  # Check podman environment and user context
  info "[DEBUG] PODMAN_HOST: ${PODMAN_HOST:-not set}"
  info "[DEBUG] CONTAINER_HOST: ${CONTAINER_HOST:-not set}"
  info "[DEBUG] Current user: $(whoami)"
  info "[DEBUG] Current UID: $(id -u)"
  
  # Check for devcontainer-specific environment
  info "[DEBUG] REMOTE_CONTAINERS_IPC: ${REMOTE_CONTAINERS_IPC:-not set}"
  info "[DEBUG] DEVCONTAINER: ${DEVCONTAINER:-not set}"
  
  # Check if we're running inside a container (which would explain why we can't see all containers)
  INSIDE_CONTAINER=false
  if [[ -f /.dockerenv ]] || [[ -n "${container:-}" ]]; then
    INSIDE_CONTAINER=true
    info "[DEBUG] Script appears to be running inside a container"
  fi
  # Also check hostname - if it matches container name, we're likely inside that container
  HOSTNAME=$(hostname 2>/dev/null || echo "")
  info "[DEBUG] Hostname: ${HOSTNAME}"
  if [[ "$HOSTNAME" == "zms-web" ]] || [[ "$HOSTNAME" == *"zms-web"* ]]; then
    INSIDE_CONTAINER=true
    info "[DEBUG] Hostname suggests we're running inside zms-web container"
  fi
  
  # Check all containers (including stopped) to see what actually exists - this is the source of truth
  PODMAN_PS_ALL=$("$PODMAN_EXEC" ps -a --format "{{.Names}}" 2>&1 || echo "")
  info "[DEBUG] podman ps -a (all containers) names: '${PODMAN_PS_ALL}'"
  
  # If ps -a worked and shows containers, extract the names we need
  if [[ -n "$PODMAN_PS_ALL" ]] && [[ "$PODMAN_PS_ALL" != *"Error"* ]] && [[ ${#PODMAN_PS_ALL} -gt 10 ]]; then
    info "[DEBUG] podman ps -a returned output, extracting container names..."
    # Extract zms-web and zms-db from the output
    ZMS_WEB_FOUND=$(echo "$PODMAN_PS_ALL" | grep -E '^zms-web$|^zms-web[[:space:]]|zms-web$' || echo "")
    ZMS_DB_FOUND=$(echo "$PODMAN_PS_ALL" | grep -E '^zms-db$|^zms-db[[:space:]]|zms-db$' || echo "")
    info "[DEBUG] Found in ps -a: zms-web='${ZMS_WEB_FOUND}', zms-db='${ZMS_DB_FOUND}'"
    
    if [[ -n "$ZMS_WEB_FOUND" ]] && [[ -n "$ZMS_DB_FOUND" ]]; then
      info "[DEBUG] Both containers found in podman ps -a, using that as source of truth"
      CONTAINER_NAMES="zms-web"$'\n'"zms-db"
    elif [[ -n "$ZMS_DB_FOUND" ]] && [[ "$INSIDE_CONTAINER" == true ]]; then
      # If we're inside a container and can see zms-db but not zms-web, we're likely inside zms-web
      info "[DEBUG] Running inside container, can see zms-db but not zms-web - assuming we're inside zms-web"
      CONTAINER_NAMES="zms-web"$'\n'"zms-db"
    elif [[ -n "$ZMS_WEB_FOUND" ]]; then
      CONTAINER_NAMES="zms-web"
    elif [[ -n "$ZMS_DB_FOUND" ]]; then
      CONTAINER_NAMES="zms-db"
    fi
  fi
  
  # First try: plain podman ps to see if it works at all
  PODMAN_PS_PLAIN=$("$PODMAN_EXEC" ps 2>&1)
  PODMAN_EXIT=$?
  info "[DEBUG] podman ps (plain) exit code: ${PODMAN_EXIT}"
  info "[DEBUG] Plain ps output length: ${#PODMAN_PS_PLAIN} chars"
  if [[ ${#PODMAN_PS_PLAIN} -lt 200 ]]; then
    info "[DEBUG] Plain ps output: '${PODMAN_PS_PLAIN}'"
  else
    info "[DEBUG] Plain ps output (first 200 chars): '${PODMAN_PS_PLAIN:0:200}'"
  fi
  
  # If we only got the header (79 chars = just the header line), try alternative methods
  HEADER_ONLY=false
  if [[ ${#PODMAN_PS_PLAIN} -eq 79 ]] && [[ "$PODMAN_PS_PLAIN" == *"CONTAINER ID"* ]]; then
    HEADER_ONLY=true
    info "[DEBUG] Only header found (${#PODMAN_PS_PLAIN} chars), trying alternative detection methods..."
  fi
  
  if [[ "$HEADER_ONLY" == true ]]; then
    # If podman ps only shows header, containers might be running as root or in different context
    # Try podman inspect - this works by directly querying container metadata, not listing
    info "[DEBUG] podman ps shows no containers, trying podman inspect (works even if containers are root-owned)..."
    
    HAS_WEB_INSPECT=false
    HAS_DB_INSPECT=false
    
    # Check if we're running inside a container (which might affect podman access)
    if [[ -f /.dockerenv ]] || [[ -n "${container:-}" ]]; then
      info "[DEBUG] Script appears to be running inside a container"
    fi
    
    # Capture actual error from inspect to understand what's happening
    ZMS_WEB_INSPECT_ERR=$("$PODMAN_EXEC" inspect zms-web 2>&1)
    ZMS_WEB_INSPECT_EXIT=$?
    if [[ $ZMS_WEB_INSPECT_EXIT -eq 0 ]]; then
      info "[DEBUG] zms-web container exists (via inspect)"
      HAS_WEB_INSPECT=true
      CONTAINER_NAMES="zms-web"
    else
      info "[DEBUG] zms-web inspect failed (exit $ZMS_WEB_INSPECT_EXIT): ${ZMS_WEB_INSPECT_ERR:0:150}"
      # Try exec as fallback - if we can exec into it, it exists and is accessible
      ZMS_WEB_EXEC_ERR=$("$PODMAN_EXEC" exec zms-web echo "test" 2>&1)
      ZMS_WEB_EXEC_EXIT=$?
      if [[ $ZMS_WEB_EXEC_EXIT -eq 0 ]]; then
        info "[DEBUG] zms-web is accessible via exec (even though inspect failed)"
        HAS_WEB_INSPECT=true
        if [[ -z "$CONTAINER_NAMES" ]]; then
          CONTAINER_NAMES="zms-web"
        fi
      else
        info "[DEBUG] zms-web exec also failed (exit $ZMS_WEB_EXEC_EXIT): ${ZMS_WEB_EXEC_ERR:0:150}"
      fi
    fi
    
    if "$PODMAN_EXEC" inspect zms-db >/dev/null 2>&1; then
      info "[DEBUG] zms-db container exists (via inspect)"
      HAS_DB_INSPECT=true
      if [[ -n "$CONTAINER_NAMES" ]]; then
        if [[ "$CONTAINER_NAMES" != *"zms-db"* ]]; then
          CONTAINER_NAMES="$CONTAINER_NAMES"$'\n'"zms-db"
        fi
      else
        CONTAINER_NAMES="zms-db"
      fi
    else
      info "[DEBUG] zms-db container NOT found via inspect"
    fi
    
    # If inspect also fails, containers might be root-owned - try sudo
    if [[ -z "$CONTAINER_NAMES" ]]; then
      info "[DEBUG] Inspect failed, checking if containers are root-owned (trying sudo podman ps)..."
      SUDO_PS=$("$PODMAN_EXEC" ps 2>&1 || echo "")
      # Check if we can use sudo (without password prompt in script)
      if command -v sudo >/dev/null 2>&1; then
        SUDO_PODMAN_PS=$(sudo "$PODMAN_EXEC" ps --format "{{.Names}}" 2>&1 || echo "")
        if [[ -n "$SUDO_PODMAN_PS" ]] && [[ "$SUDO_PODMAN_PS" != *"Error"* ]]; then
          info "[DEBUG] Found containers via sudo podman ps: '${SUDO_PODMAN_PS}'"
          CONTAINER_NAMES="$SUDO_PODMAN_PS"
        fi
      fi
    fi
  fi
  
  # Try with format flag (only if we haven't found containers via filter yet)
  if [[ -z "$CONTAINER_NAMES" ]]; then
    CONTAINER_NAMES=$("$PODMAN_EXEC" ps --format "{{.Names}}" 2>&1)
    PODMAN_EXIT=$?
    info "[DEBUG] podman ps --format exit code: ${PODMAN_EXIT}"
    info "[DEBUG] Format output length: ${#CONTAINER_NAMES} chars"
    info "[DEBUG] Format output: '${CONTAINER_NAMES}'"
  else
    PODMAN_EXIT=0
    info "[DEBUG] Skipping format check, already found containers via filter"
  fi
  
  # If format didn't work, try parsing the plain output
  if [[ -z "$CONTAINER_NAMES" ]] || [[ "$CONTAINER_NAMES" == *"Error"* ]] || [[ "$CONTAINER_NAMES" == *"error"* ]]; then
    info "[DEBUG] Format method failed, trying to parse plain ps output..."
    if [[ $PODMAN_EXIT -eq 0 ]] && [[ -n "$PODMAN_PS_PLAIN" ]] && [[ ${#PODMAN_PS_PLAIN} -gt 79 ]]; then
      # Extract NAMES column (last column in podman ps output, skip header)
      CONTAINER_NAMES=$(echo "$PODMAN_PS_PLAIN" | tail -n +2 | awk '{print $NF}' | grep -v "^$")
      info "[DEBUG] Extracted names from plain output: '${CONTAINER_NAMES}'"
    fi
  fi
  
  if [[ $PODMAN_EXIT -eq 0 ]] && [[ -n "$CONTAINER_NAMES" ]] && [[ "$CONTAINER_NAMES" != *"Error"* ]] && [[ "$CONTAINER_NAMES" != *"error"* ]] && [[ "$CONTAINER_NAMES" != *"Cannot connect"* ]]; then
    # Normalize whitespace and check for exact matches
    # Use tr to normalize all whitespace to single spaces, then check for whole word matches
    NORMALIZED=$(echo "$CONTAINER_NAMES" | tr '\n' ' ' | tr -s '[:space:]' ' ')
    info "[DEBUG] Normalized container names: '${NORMALIZED}'"
    
    HAS_WEB=false
    HAS_DB=false
    if echo "$NORMALIZED" | grep -qw "zms-web"; then
      HAS_WEB=true
      info "[DEBUG] Found zms-web container"
    else
      info "[DEBUG] zms-web container NOT found"
    fi
    if echo "$NORMALIZED" | grep -qw "zms-db"; then
      HAS_DB=true
      info "[DEBUG] Found zms-db container"
    else
      info "[DEBUG] zms-db container NOT found"
    fi
    
    if [[ "$HAS_WEB" == true ]] && [[ "$HAS_DB" == true ]]; then
      USE_PODMAN=true
      info "Using Podman containers (zms-web, zms-db)"
      # Set default endpoints for Podman
      BASE_URI="${BASE_URI:-http://zms-web/terminvereinbarung/api/2}"
      CITIZEN_API_BASE_URI="${CITIZEN_API_BASE_URI:-http://zms-web/terminvereinbarung/api/citizen}"
    else
      info "[DEBUG] Not using Podman: HAS_WEB=${HAS_WEB}, HAS_DB=${HAS_DB}"
    fi
  else
    info "[DEBUG] No container names found or podman ps failed/errored"
  fi
else
  info "[DEBUG] Podman command not found via command -v or direct call"
fi

if [[ "$USE_PODMAN" == false ]]; then
  require_cmd ddev
  info "Using DDEV"
  # Set default endpoints for DDEV
  BASE_URI="${BASE_URI:-http://web/terminvereinbarung/api/2}"
  CITIZEN_API_BASE_URI="${CITIZEN_API_BASE_URI:-http://web/terminvereinbarung/api/citizen}"
fi

require_cmd docker

restore_db() {
  if [[ -f "$BACKUP_FILE" ]]; then
    info "Restoring database from $BACKUP_FILE ..."
    if [[ "$USE_PODMAN" == true ]]; then
      gunzip < "$BACKUP_FILE" | podman exec -i zms-db mysql -u root -proot db >/dev/null
    else
      (cd "$PROJECT_ROOT" && ddev import-db -f "$BACKUP_FILE" >/dev/null)
    fi
    info "Database restored."
  else
    warn "No backup file found to restore."
  fi
}

trap 'warn "Failure detected, attempting DB restore..."; restore_db' ERR

# 1) Backup current DB
info "Exporting current database to $BACKUP_FILE ..."
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysqldump -u root -proot db | gzip > "$BACKUP_FILE")
else
  (cd "$PROJECT_ROOT" && ddev export-db -f "$BACKUP_FILE" >/dev/null)
fi
info "Backup completed."

# 1.5) Clear application caches inside container (zmsapi, zmsdb, zmscitizenapi)
info "Clearing application caches (zmsapi, zmsdb, zmscitizenapi) using CACHE_DIR..."
if [[ "$USE_PODMAN" == true ]]; then
  CACHE_DIR=$(cd "$PROJECT_ROOT" && podman exec zms-web bash -lc 'echo "$CACHE_DIR"' 2>/dev/null || echo "")
else
  CACHE_DIR=$(cd "$PROJECT_ROOT" && ddev exec bash -lc 'echo "$CACHE_DIR"' 2>/dev/null || echo "")
fi
if [[ -n "$CACHE_DIR" ]]; then
  info "CACHE_DIR=$CACHE_DIR"
  for dir in zmsapi zmsdb zmscitizenapi; do
    if [[ "$USE_PODMAN" == true ]]; then
      (cd "$PROJECT_ROOT" && podman exec zms-web bash -lc "
        if [ -d \"$CACHE_DIR/$dir\" ]; then
          rm -rf \"$CACHE_DIR/$dir\"/* \"$CACHE_DIR/$dir\"/.[!.]* \"$CACHE_DIR/$dir\"/..?* 2>/dev/null || true
          echo \"[CACHE] cleared $CACHE_DIR/$dir\"
        fi
      " || true)
    else
      (cd "$PROJECT_ROOT" && ddev exec bash -lc "
        if [ -d \"$CACHE_DIR/$dir\" ]; then
          rm -rf \"$CACHE_DIR/$dir\"/* \"$CACHE_DIR/$dir\"/.[!.]* \"$CACHE_DIR/$dir\"/..?* 2>/dev/null || true
          echo \"[CACHE] cleared $CACHE_DIR/$dir\"
        fi
      ")
    fi
  done
  # Also clear host-side cache that corresponds to container path (\"/var/www/html\" maps to project root)
  HOST_CACHE_DIR="$CACHE_DIR"
  if [[ "$HOST_CACHE_DIR" == /var/www/html/* ]]; then
    HOST_CACHE_DIR="$PROJECT_ROOT${HOST_CACHE_DIR#/var/www/html}"
    if [[ -d "$HOST_CACHE_DIR" ]]; then
      rm -rf "$HOST_CACHE_DIR"/* "$HOST_CACHE_DIR"/.[!.]* "$HOST_CACHE_DIR"/..?* 2>/dev/null || true
      info "[CACHE][host] cleared $HOST_CACHE_DIR"
    fi
    for dir in zmsapi zmsdb zmscitizenapi; do
      if [[ -d "$HOST_CACHE_DIR/$dir" ]]; then
        rm -rf "$HOST_CACHE_DIR/$dir"/* "$HOST_CACHE_DIR/$dir"/.[!.]* "$HOST_CACHE_DIR/$dir"/..?* 2>/dev/null || true
        info "[CACHE][host] cleared $HOST_CACHE_DIR/$dir"
      fi
    done
  fi
else
  if [[ "$USE_PODMAN" == true ]]; then
    warn "CACHE_DIR not found in podman container"
  else
    warn "CACHE_DIR not found in ddev container"
  fi
fi

# 2) Drop all tables to ensure clean state (removes tables created by migrations from previous runs)
info "Dropping all existing tables to ensure clean state..."
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
  SET FOREIGN_KEY_CHECKS = 0;
  SET @tables = NULL;
  SELECT GROUP_CONCAT('\\`', table_name, '\\`')
    INTO @tables
    FROM information_schema.tables
    WHERE table_schema = 'db' AND table_type = 'BASE TABLE';
  SET @tables = IFNULL(@tables, 'dummy');
  SET @tables = CONCAT('DROP TABLE IF EXISTS ', @tables);
  PREPARE stmt FROM @tables;
  EXECUTE stmt;
  DEALLOCATE PREPARE stmt;
  SET FOREIGN_KEY_CHECKS = 1;
" >/dev/null 2>&1 || echo "No tables to drop or database is empty")
else
  (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
    SET FOREIGN_KEY_CHECKS = 0;
    SET @tables = NULL;
    SELECT GROUP_CONCAT('\\`', table_name, '\\`')
      INTO @tables
      FROM information_schema.tables
      WHERE table_schema = 'db' AND table_type = 'BASE TABLE';
    SET @tables = IFNULL(@tables, 'dummy');
    SET @tables = CONCAT('DROP TABLE IF EXISTS ', @tables);
    PREPARE stmt FROM @tables;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    SET FOREIGN_KEY_CHECKS = 1;
  " >/dev/null 2>&1 || echo "No tables to drop or database is empty")
fi
info "Tables dropped (if any existed)."

# 3) Import base database
if [[ -f "$BASE_SQL_FILE" ]]; then
  info "Importing base database from $BASE_SQL_FILE ..."
  if [[ "$USE_PODMAN" == true ]]; then
    (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db < "$BASE_SQL_FILE" >/dev/null)
  else
    (cd "$PROJECT_ROOT" && ddev import-db --file="$BASE_SQL_FILE" >/dev/null)
  fi
  info "Base database import completed."
  
  # Log database tables after initial import
  info "Database tables after initial SQL import:"
  if [[ "$USE_PODMAN" == true ]]; then
    (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
    SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
    FROM information_schema.tables
    WHERE table_schema = 'db'
    ORDER BY table_name;
  " || true)
  else
    (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
    SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
    FROM information_schema.tables
    WHERE table_schema = 'db'
    ORDER BY table_name;
  " || true)
  fi
else
  warn "Base SQL file not found at $BASE_SQL_FILE, skipping import."
fi

# 4) Flyway BEFORE PHP migrations
info "Running Flyway migrations..."
if [[ "$USE_PODMAN" == true ]]; then
  # Discover the actual podman network used by zms-db
  PODMAN_NETWORK=$(podman inspect -f '{{range $k,$v := .NetworkSettings.Networks}}{{printf "%s" $k}}{{end}}' zms-db 2>/dev/null || true)
  if [[ -z "$PODMAN_NETWORK" ]]; then
    PODMAN_NETWORK="devcontainer_default"
  fi

  # Wait for MySQL to be ready inside zms-db (max ~30s)
  info "Waiting for MySQL (zms-db) to become ready..."
  for i in {1..30}; do
    if (cd "$PROJECT_ROOT" && podman exec zms-db sh -lc 'mysqladmin ping -uroot -proot --silent' >/dev/null 2>&1); then
      break
    fi
    sleep 1
  done

  # Run Flyway in a podman container on the discovered network
  info "Running Flyway via Podman on network ($PODMAN_NETWORK)..."
  podman run --rm \
    --network "$PODMAN_NETWORK" \
    -v "$PROJECT_ROOT/zmsautomation/flyway:/flyway/sql:ro" \
    docker.io/flyway/flyway:9 \
    -url=jdbc:mysql://db:3306/db -user=root -password=root \
    -locations=filesystem:/flyway/sql \
    -baselineOnMigrate=true \
    migrate || warn "Flyway (Podman) failed; continuing."
else
  # Try ddev service 'flyway' first (if you add one); otherwise run Flyway via Docker
  if (cd "$PROJECT_ROOT" && ddev exec -s flyway flyway -v >/dev/null 2>&1); then
    info "Using ddev flyway service"
    (cd "$PROJECT_ROOT" && ddev exec -s flyway flyway \
      -url=jdbc:mysql://db:3306/db -user=db -password=db \
      -locations=filesystem:/var/www/html/zmsautomation/flyway \
      -baselineOnMigrate=true migrate)
  else
    # Get ddev network name
    DDEV_NETWORK=$(cd "$PROJECT_ROOT" && ddev describe -j 2>/dev/null | grep -o '"network": "[^"]*"' | cut -d '"' -f4 || echo "ddev_default")
    if [[ -z "$DDEV_NETWORK" ]]; then
      DDEV_NETWORK="ddev_default"
    fi
    
    # Run Flyway in a Docker container on the ddev network
    info "Running Flyway via Docker on ddev network ($DDEV_NETWORK)..."
    docker run --rm \
      --network "$DDEV_NETWORK" \
      -v "$PROJECT_ROOT/zmsautomation/flyway:/flyway/sql" \
      flyway/flyway:9 \
      -url=jdbc:mysql://db:3306/db -user=db -password=db \
      -locations=filesystem:/flyway/sql \
      -baselineOnMigrate=true \
      migrate || warn "Flyway (Docker on ddev network) failed; continuing."
  fi
fi

# Log database tables after Flyway
info "Database tables after Flyway (before PHP migrations):"
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
  SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
  FROM information_schema.tables
  WHERE table_schema = 'db'
  ORDER BY table_name;
" || true)
  else
    (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
  SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
  FROM information_schema.tables
  WHERE table_schema = 'db'
  ORDER BY table_name;
" || true)
  fi
info "flyway_schema_history (post-flyway):"
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
    SELECT installed_rank, version, description, type, installed_by, success, installed_on
    FROM flyway_schema_history
    ORDER BY installed_rank;
  " || true)
else
  (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
    SELECT installed_rank, version, description, type, installed_by, success, installed_on
    FROM flyway_schema_history
    ORDER BY installed_rank;
  " || true)
fi

# 5) Run PHP migrations
info "Running zmsapi PHP migrations..."
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec zms-web bash -lc "cd zmsapi && vendor/bin/migrate --update")
else
  (cd "$PROJECT_ROOT" && ddev exec zmsapi/vendor/bin/migrate --update)
fi
info "PHP migrations completed."

# Log database tables after PHP migrations
info "Database tables after PHP migrations:"
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
  SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
  FROM information_schema.tables
  WHERE table_schema = 'db'
  ORDER BY table_name;
" || true)
  else
    (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
  SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
  FROM information_schema.tables
  WHERE table_schema = 'db'
  ORDER BY table_name;
" || true)
  fi
info "flyway_schema_history (post-php-migrations):"
if [[ "$USE_PODMAN" == true ]]; then
  (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
    SELECT installed_rank, version, description, type, installed_by, success, installed_on
    FROM flyway_schema_history
    ORDER BY installed_rank;
  " || true)
else
  (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
    SELECT installed_rank, version, description, type, installed_by, success, installed_on
    FROM flyway_schema_history
    ORDER BY installed_rank;
  " || true)
fi

# 6) Run hourly (best-effort)
info "Running hourly (best-effort)..."
if [[ "$USE_PODMAN" == true ]]; then
  if ! (cd "$PROJECT_ROOT" && podman exec zms-web bash -lc "zmsapi/cron/cronjob.hourly --city=munich"); then
    warn "Hourly command failed or not available; continuing."
  else
    info "Hourly command completed."
    
    # Log database tables after hourly
    info "Database tables after hourly cron:"
    (cd "$PROJECT_ROOT" && podman exec -i zms-db mysql -u root -proot db -e "
      SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
      FROM information_schema.tables
      WHERE table_schema = 'db'
      ORDER BY table_name;
    " || true)
  fi
else
  if ! (cd "$PROJECT_ROOT" && ddev exec zmsapi/cron/cronjob.hourly --city=munich); then
    warn "Hourly command failed or not available; continuing."
  else
    info "Hourly command completed."
    
    # Log database tables after hourly
    info "Database tables after hourly cron:"
    (cd "$PROJECT_ROOT" && ddev exec mysql -udb -pdb db -e "
      SELECT table_name AS 'Table Name', table_rows AS 'Row Count'
      FROM information_schema.tables
      WHERE table_schema = 'db'
      ORDER BY table_name;
    " || true)
  fi
fi

# 7) Pings for readiness
if [[ "$USE_PODMAN" == true ]]; then
  # Note: These pings run from the host, so use localhost with port
  info "Pinging citizenapi: http://localhost:8090/terminvereinbarung/api/citizen/offices-and-services/"
  if curl -sf --max-time 10 "http://localhost:8090/terminvereinbarung/api/citizen/offices-and-services/" >/dev/null; then
    info "✓ citizenapi offices-and-services responding"
  else
    warn "✗ citizenapi offices-and-services NOT responding"
  fi

  info "Pinging zmsapi: http://localhost:8090/terminvereinbarung/api/2/status/"
  if curl -sf --max-time 10 "http://localhost:8090/terminvereinbarung/api/2/status/" >/dev/null; then
    info "✓ zmsapi status responding"
  else
    warn "✗ zmsapi status NOT responding"
  fi
else
  # Note: These pings run from the host, so use zms.ddev.site
  info "Pinging citizenapi: http://zms.ddev.site/terminvereinbarung/api/citizen/offices-and-services/"
  if curl -sf --max-time 10 "http://zms.ddev.site/terminvereinbarung/api/citizen/offices-and-services/" >/dev/null; then
    info "✓ citizenapi offices-and-services responding"
  else
    warn "✗ citizenapi offices-and-services NOT responding"
  fi

  info "Pinging zmsapi: http://zms.ddev.site/terminvereinbarung/api/2/status/"
  if curl -sf --max-time 10 "http://zms.ddev.site/terminvereinbarung/api/2/status/" >/dev/null; then
    info "✓ zmsapi status responding"
  else
    warn "✗ zmsapi status NOT responding"
  fi
fi

# 8) Run tests (in a throwaway Maven container, using cached ~/.m2)
info "Running REST-assured tests..."
docker volume create "$M2_CACHE_VOLUME" >/dev/null 2>&1 || true
docker volume create "$TARGET_VOLUME" >/dev/null 2>&1 || true

if [[ "$USE_PODMAN" == true ]]; then
  # Get podman network name from docker-compose network (same logic as Flyway)
  PODMAN_NETWORK=$(docker network ls --filter "name=devcontainer" --format "{{.Name}}" | head -1 || echo "devcontainer_default")
  if [[ -z "$PODMAN_NETWORK" ]]; then
    PODMAN_NETWORK="devcontainer_default"
  fi
  
  # Run tests on podman network so container can reach podman services
  docker run --rm \
    --network "$PODMAN_NETWORK" \
    -e BASE_URI="$BASE_URI" \
    -e CITIZEN_API_BASE_URI="$CITIZEN_API_BASE_URI" \
    -v "$PROJECT_ROOT:/workspace:ro" \
    -v "$M2_CACHE_VOLUME:/root/.m2" \
    -v "$TARGET_VOLUME:/workspace/zmsautomation/target" \
    -w /workspace/zmsautomation \
    "$MAVEN_IMAGE" mvn -B -DtrimStackTrace=false test
else
  # Get ddev network name (same logic as Flyway)
  DDEV_NETWORK=$(cd "$PROJECT_ROOT" && ddev describe -j 2>/dev/null | grep -o '"network": "[^"]*"' | cut -d '"' -f4 || echo "ddev_default")
  if [[ -z "$DDEV_NETWORK" ]]; then
    DDEV_NETWORK="ddev_default"
  fi
  
  # Run tests on ddev network so container can reach ddev services
  docker run --rm \
    --network "$DDEV_NETWORK" \
    -e BASE_URI="$BASE_URI" \
    -e CITIZEN_API_BASE_URI="$CITIZEN_API_BASE_URI" \
    -v "$PROJECT_ROOT:/workspace:ro" \
    -v "$M2_CACHE_VOLUME:/root/.m2" \
    -v "$TARGET_VOLUME:/workspace/zmsautomation/target" \
    -w /workspace/zmsautomation \
    "$MAVEN_IMAGE" mvn -B -DtrimStackTrace=false test
fi

TEST_EXIT=$?

# Print surefire reports (if any)
info "Collecting surefire reports..."
docker run --rm -v "$TARGET_VOLUME:/t" alpine sh -lc '
  if [ -d /t/surefire-reports ]; then
    ls -la /t/surefire-reports;
    for f in /t/surefire-reports/*.txt; do
      echo "==== $f ===="; cat "$f"; echo; done || true;
  else
    echo "No surefire-reports found";
  fi'

# 9) Restore DB regardless of test result
info "Restoring original database..."
restore_db

if [[ $TEST_EXIT -eq 0 ]]; then
  info "Tests completed successfully."
else
  warn "Tests failed (exit $TEST_EXIT)."
fi

exit "$TEST_EXIT"


